{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad730a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T05:23:15.307336Z",
     "iopub.status.busy": "2025-01-19T05:23:15.307051Z",
     "iopub.status.idle": "2025-01-19T05:59:11.300445Z",
     "shell.execute_reply": "2025-01-19T05:59:11.299454Z"
    },
    "papermill": {
     "duration": 2155.99721,
     "end_time": "2025-01-19T05:59:11.302007",
     "exception": false,
     "start_time": "2025-01-19T05:23:15.304797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5131 images belonging to 5 classes.\n",
      "Found 484 images belonging to 5 classes.\n",
      "Found 482 images belonging to 5 classes.\n",
      "Training VGG16...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 347ms/step - accuracy: 0.3383 - loss: 1.4450 - val_accuracy: 0.5826 - val_loss: 0.9524\n",
      "Epoch 2/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.5662 - loss: 1.0154 - val_accuracy: 0.6302 - val_loss: 0.8836\n",
      "Epoch 3/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 232ms/step - accuracy: 0.6852 - loss: 0.7687 - val_accuracy: 0.6591 - val_loss: 0.8363\n",
      "Epoch 4/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.7456 - loss: 0.6576 - val_accuracy: 0.7479 - val_loss: 0.6975\n",
      "Epoch 5/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.7744 - loss: 0.5770 - val_accuracy: 0.7583 - val_loss: 0.6498\n",
      "Epoch 6/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.7955 - loss: 0.5409 - val_accuracy: 0.7624 - val_loss: 0.6679\n",
      "Epoch 7/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.8238 - loss: 0.4601 - val_accuracy: 0.7645 - val_loss: 0.6533\n",
      "Epoch 8/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.8289 - loss: 0.4547 - val_accuracy: 0.7541 - val_loss: 0.6449\n",
      "Epoch 9/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.8469 - loss: 0.3854 - val_accuracy: 0.7603 - val_loss: 0.6519\n",
      "Epoch 10/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.8664 - loss: 0.3474 - val_accuracy: 0.7521 - val_loss: 0.6954\n",
      "Training InceptionV3...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 411ms/step - accuracy: 0.5632 - loss: 1.0686 - val_accuracy: 0.7025 - val_loss: 0.7911\n",
      "Epoch 2/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 144ms/step - accuracy: 0.8307 - loss: 0.4665 - val_accuracy: 0.7025 - val_loss: 0.7721\n",
      "Epoch 3/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 143ms/step - accuracy: 0.8891 - loss: 0.3044 - val_accuracy: 0.6839 - val_loss: 0.8119\n",
      "Epoch 4/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 145ms/step - accuracy: 0.8980 - loss: 0.2677 - val_accuracy: 0.7169 - val_loss: 0.9374\n",
      "Epoch 5/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 144ms/step - accuracy: 0.9189 - loss: 0.1972 - val_accuracy: 0.6983 - val_loss: 1.1069\n",
      "Epoch 6/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 144ms/step - accuracy: 0.9233 - loss: 0.1659 - val_accuracy: 0.6901 - val_loss: 1.1652\n",
      "Epoch 7/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 144ms/step - accuracy: 0.9217 - loss: 0.1614 - val_accuracy: 0.6880 - val_loss: 1.1920\n",
      "Epoch 8/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 144ms/step - accuracy: 0.9206 - loss: 0.1743 - val_accuracy: 0.7231 - val_loss: 1.0282\n",
      "Epoch 9/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 143ms/step - accuracy: 0.9077 - loss: 0.1976 - val_accuracy: 0.7045 - val_loss: 1.2764\n",
      "Epoch 10/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 143ms/step - accuracy: 0.9222 - loss: 0.1504 - val_accuracy: 0.7025 - val_loss: 1.3347\n",
      "Training ResNet50...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 329ms/step - accuracy: 0.6273 - loss: 0.9431 - val_accuracy: 0.2789 - val_loss: 2.4420\n",
      "Epoch 2/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 160ms/step - accuracy: 0.8428 - loss: 0.4149 - val_accuracy: 0.2149 - val_loss: 3.2329\n",
      "Epoch 3/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 158ms/step - accuracy: 0.8934 - loss: 0.2808 - val_accuracy: 0.2624 - val_loss: 3.6056\n",
      "Epoch 4/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 159ms/step - accuracy: 0.9173 - loss: 0.2203 - val_accuracy: 0.2314 - val_loss: 4.8955\n",
      "Epoch 5/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 158ms/step - accuracy: 0.9223 - loss: 0.1786 - val_accuracy: 0.3616 - val_loss: 4.4516\n",
      "Epoch 6/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 157ms/step - accuracy: 0.9193 - loss: 0.1785 - val_accuracy: 0.6260 - val_loss: 2.4154\n",
      "Epoch 7/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 158ms/step - accuracy: 0.9253 - loss: 0.1588 - val_accuracy: 0.6818 - val_loss: 1.8561\n",
      "Epoch 8/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 159ms/step - accuracy: 0.9194 - loss: 0.1734 - val_accuracy: 0.7376 - val_loss: 1.1167\n",
      "Epoch 9/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 160ms/step - accuracy: 0.9293 - loss: 0.1323 - val_accuracy: 0.7190 - val_loss: 1.5250\n",
      "Epoch 10/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 158ms/step - accuracy: 0.9257 - loss: 0.1492 - val_accuracy: 0.7107 - val_loss: 1.2907\n",
      "Training MobileNetV2...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 236ms/step - accuracy: 0.5569 - loss: 1.0805 - val_accuracy: 0.3140 - val_loss: 2.0348\n",
      "Epoch 2/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.8052 - loss: 0.5089 - val_accuracy: 0.3905 - val_loss: 2.1922\n",
      "Epoch 3/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.8858 - loss: 0.3042 - val_accuracy: 0.4380 - val_loss: 2.4355\n",
      "Epoch 4/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9071 - loss: 0.2590 - val_accuracy: 0.5041 - val_loss: 1.7788\n",
      "Epoch 5/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9171 - loss: 0.2172 - val_accuracy: 0.4959 - val_loss: 2.2936\n",
      "Epoch 6/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9208 - loss: 0.2030 - val_accuracy: 0.6302 - val_loss: 1.3065\n",
      "Epoch 7/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9273 - loss: 0.1913 - val_accuracy: 0.6550 - val_loss: 1.2241\n",
      "Epoch 8/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.9265 - loss: 0.1624 - val_accuracy: 0.6136 - val_loss: 1.7407\n",
      "Epoch 9/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9270 - loss: 0.1613 - val_accuracy: 0.6488 - val_loss: 1.4136\n",
      "Epoch 10/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.9320 - loss: 0.1374 - val_accuracy: 0.6653 - val_loss: 1.5046\n",
      "Training InceptionResNetV2...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 795ms/step - accuracy: 0.6194 - loss: 0.9627 - val_accuracy: 0.7190 - val_loss: 0.7250\n",
      "Epoch 2/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 316ms/step - accuracy: 0.8439 - loss: 0.4219 - val_accuracy: 0.7521 - val_loss: 0.6857\n",
      "Epoch 3/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 315ms/step - accuracy: 0.8918 - loss: 0.2735 - val_accuracy: 0.7293 - val_loss: 0.7744\n",
      "Epoch 4/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 315ms/step - accuracy: 0.9122 - loss: 0.2054 - val_accuracy: 0.7211 - val_loss: 0.8690\n",
      "Epoch 5/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 317ms/step - accuracy: 0.9110 - loss: 0.1931 - val_accuracy: 0.7128 - val_loss: 1.0691\n",
      "Epoch 6/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 317ms/step - accuracy: 0.9303 - loss: 0.1591 - val_accuracy: 0.7231 - val_loss: 1.4241\n",
      "Epoch 7/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 316ms/step - accuracy: 0.9252 - loss: 0.1426 - val_accuracy: 0.6694 - val_loss: 1.0998\n",
      "Epoch 8/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 315ms/step - accuracy: 0.9254 - loss: 0.1536 - val_accuracy: 0.7231 - val_loss: 1.6564\n",
      "Epoch 9/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 315ms/step - accuracy: 0.9314 - loss: 0.1313 - val_accuracy: 0.6839 - val_loss: 1.6227\n",
      "Epoch 10/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 316ms/step - accuracy: 0.9201 - loss: 0.1584 - val_accuracy: 0.7190 - val_loss: 1.5890\n",
      "Evaluating VGG16...\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "Accuracy: 0.7863070539419087\n",
      "Precision: 0.7950364716427659\n",
      "Recall: 0.7863070539419087\n",
      "F1 Score: 0.7848148280476595\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       0.95      0.94      0.94       151\n",
      "            Glaucoma       0.67      0.79      0.73       135\n",
      "             Healthy       0.72      0.74      0.73       102\n",
      "        Macular Scar       0.78      0.66      0.72        44\n",
      "              Myopia       0.84      0.52      0.64        50\n",
      "\n",
      "            accuracy                           0.79       482\n",
      "           macro avg       0.79      0.73      0.75       482\n",
      "        weighted avg       0.80      0.79      0.78       482\n",
      "\n",
      "Evaluating InceptionV3...\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step\n",
      "Accuracy: 0.7219917012448133\n",
      "Precision: 0.7150579606345614\n",
      "Recall: 0.7219917012448133\n",
      "F1 Score: 0.7160878422717505\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       0.88      0.96      0.92       151\n",
      "            Glaucoma       0.64      0.52      0.57       135\n",
      "             Healthy       0.66      0.72      0.69       102\n",
      "        Macular Scar       0.61      0.64      0.62        44\n",
      "              Myopia       0.62      0.64      0.63        50\n",
      "\n",
      "            accuracy                           0.72       482\n",
      "           macro avg       0.68      0.69      0.69       482\n",
      "        weighted avg       0.72      0.72      0.72       482\n",
      "\n",
      "Evaluating ResNet50...\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step\n",
      "Accuracy: 0.7323651452282157\n",
      "Precision: 0.7450101677461965\n",
      "Recall: 0.7323651452282157\n",
      "F1 Score: 0.7372475011931766\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       0.99      0.88      0.93       151\n",
      "            Glaucoma       0.64      0.69      0.66       135\n",
      "             Healthy       0.66      0.65      0.65       102\n",
      "        Macular Scar       0.64      0.66      0.65        44\n",
      "              Myopia       0.56      0.64      0.60        50\n",
      "\n",
      "            accuracy                           0.73       482\n",
      "           macro avg       0.70      0.70      0.70       482\n",
      "        weighted avg       0.75      0.73      0.74       482\n",
      "\n",
      "Evaluating MobileNetV2...\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n",
      "Accuracy: 0.6846473029045643\n",
      "Precision: 0.7336711569651171\n",
      "Recall: 0.6846473029045643\n",
      "F1 Score: 0.6907634104573426\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       1.00      0.68      0.81       151\n",
      "            Glaucoma       0.63      0.64      0.64       135\n",
      "             Healthy       0.62      0.81      0.71       102\n",
      "        Macular Scar       0.47      0.80      0.59        44\n",
      "              Myopia       0.67      0.44      0.53        50\n",
      "\n",
      "            accuracy                           0.68       482\n",
      "           macro avg       0.68      0.68      0.65       482\n",
      "        weighted avg       0.73      0.68      0.69       482\n",
      "\n",
      "Evaluating InceptionResNetV2...\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step\n",
      "Accuracy: 0.7302904564315352\n",
      "Precision: 0.7337545215241875\n",
      "Recall: 0.7302904564315352\n",
      "F1 Score: 0.7295196605837886\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       0.93      0.95      0.94       151\n",
      "            Glaucoma       0.62      0.62      0.62       135\n",
      "             Healthy       0.66      0.65      0.65       102\n",
      "        Macular Scar       0.77      0.55      0.64        44\n",
      "              Myopia       0.57      0.70      0.63        50\n",
      "\n",
      "            accuracy                           0.73       482\n",
      "           macro avg       0.71      0.69      0.70       482\n",
      "        weighted avg       0.73      0.73      0.73       482\n",
      "\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step\n",
      "Ensemble Model Evaluation:\n",
      "Accuracy: 0.7800829875518672\n",
      "Precision: 0.7831447096915731\n",
      "Recall: 0.7800829875518672\n",
      "F1 Score: 0.7810370214166537\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Diabetic Retinopathy       0.97      0.95      0.96       151\n",
      "            Glaucoma       0.69      0.70      0.69       135\n",
      "             Healthy       0.68      0.74      0.70       102\n",
      "        Macular Scar       0.79      0.70      0.75        44\n",
      "              Myopia       0.69      0.66      0.67        50\n",
      "\n",
      "            accuracy                           0.78       482\n",
      "           macro avg       0.76      0.75      0.75       482\n",
      "        weighted avg       0.78      0.78      0.78       482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50, MobileNetV2, InceptionResNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Dataset paths\n",
    "dataset_dir = \"/kaggle/input/aug-v100\"\n",
    "train_folder = os.path.join(dataset_dir, \"output\")\n",
    "val_folder = os.path.join(dataset_dir, \"val\")\n",
    "test_folder = os.path.join(dataset_dir, \"test\")\n",
    "folders = [\"Diabetic Retinopathy\", \"Glaucoma\", \"Healthy\", \"Macular Scar\", \"Myopia\"]\n",
    "image_size = 224  # Input image size for ResNet50\n",
    "batch_size = 32\n",
    "num_classes = len(folders)\n",
    "\n",
    "# Data generators\n",
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = datagen.flow_from_directory(train_folder, target_size=(image_size, image_size), batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = datagen.flow_from_directory(val_folder, target_size=(image_size, image_size), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = datagen.flow_from_directory(test_folder, target_size=(image_size, image_size), batch_size=1, class_mode='categorical', shuffle=False)\n",
    "\n",
    "# Define function to create and train models\n",
    "def create_model(base_model, input_size, num_classes):\n",
    "    base = base_model(weights=\"imagenet\", include_top=False, input_shape=(input_size, input_size, 3))\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_gen, val_gen, epochs=10):\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=epochs, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Train individual models\n",
    "models = {\n",
    "    \"VGG16\": VGG16,\n",
    "    \"InceptionV3\": InceptionV3,\n",
    "    \"ResNet50\": ResNet50,\n",
    "    \"MobileNetV2\": MobileNetV2,\n",
    "    \"InceptionResNetV2\": InceptionResNetV2\n",
    "}\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model_fn in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model = create_model(model_fn, image_size, num_classes)\n",
    "    trained_model = train_model(model, train_generator, val_generator, epochs=10)\n",
    "    trained_models[model_name] = trained_model\n",
    "\n",
    "# Evaluate individual models\n",
    "def evaluate_model(model, test_gen):\n",
    "    predictions = model.predict(test_gen, verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = test_gen.classes\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    report = classification_report(y_true, y_pred, target_names=folders)\n",
    "    \n",
    "    return acc, precision, recall, f1, report\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    acc, precision, recall, f1, report = evaluate_model(model, test_generator)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(report)\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_predictions = np.zeros((test_generator.samples, num_classes))\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "ensemble_predictions /= len(trained_models)\n",
    "y_pred_ensemble = np.argmax(ensemble_predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "ensemble_acc = accuracy_score(y_true, y_pred_ensemble)\n",
    "ensemble_precision = precision_score(y_true, y_pred_ensemble, average='weighted')\n",
    "ensemble_recall = recall_score(y_true, y_pred_ensemble, average='weighted')\n",
    "ensemble_f1 = f1_score(y_true, y_pred_ensemble, average='weighted')\n",
    "ensemble_report = classification_report(y_true, y_pred_ensemble, target_names=folders)\n",
    "\n",
    "print(\"Ensemble Model Evaluation:\")\n",
    "print(f\"Accuracy: {ensemble_acc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1 Score: {ensemble_f1}\")\n",
    "print(ensemble_report)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6503486,
     "sourceId": 10505875,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2163.833035,
   "end_time": "2025-01-19T05:59:16.648127",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-19T05:23:12.815092",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
